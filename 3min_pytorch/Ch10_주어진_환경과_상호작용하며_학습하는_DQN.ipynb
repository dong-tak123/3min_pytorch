{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch10. 주어진 환경과 상호작용하며 학습하는 DQN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzjvGmsycFdh",
        "outputId": "eb23ca10-b082-4a30-dba7-8073a742182d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.1 강화학습과 DQN 기초"
      ],
      "metadata": {
        "id": "Ra3mYYLiX9ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- agent : 플레이어\n",
        "- state : agent가 활동하는 환경\n",
        "- action : agent가 시행하는 상호작용\n",
        "- reward : agent의 action에 따른 점수 혹은 결과"
      ],
      "metadata": {
        "id": "BOymPuvtYOTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> agent가 state에서 action을 반복하며 최고의 reward를 받는 방향으로 성장한다.."
      ],
      "metadata": {
        "id": "EUxeTwzpYjMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.2 카트폴 게임 마스터하기\n",
        "\n",
        "- 보통 강화학습이 게임에 적용된다..\n",
        "- 카트 위의 막대기가 오래 세워져 있도록 카트를 움직이는 게임.."
      ],
      "metadata": {
        "id": "UP1pdh_iYBJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym      #게임환경 제공\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque       #데크..양쪽에서 삽입 및 삭제 가능..\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3aqFVKUjcNkz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.1 하이퍼파라미터"
      ],
      "metadata": {
        "id": "Kwy4d_N5X9ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "agent가 처음에는 무작위로 행동하다가 점점 경험을 학습하면서 좋은 방향으로 행동하게끔 함.."
      ],
      "metadata": {
        "id": "EP_r-8DRc4CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**할인계수**\n",
        "\n",
        "- 0 ~ 1 사이의 숫자\n",
        "- 0에 가까울수록 현재의 보상이 더 중요..\n",
        "- 1에 가까울수록 미래의 보상도 현재의 보상만큼 중요하게 고려됨.."
      ],
      "metadata": {
        "id": "nMTSZSMueKml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 50    #게임 횟수\n",
        "\n",
        "#학습 중 에이전트가 무작위로 행동할 확률..\n",
        "EPS_START = 0.9         #시작\n",
        "EPS_END = 0.05          #끝\n",
        "\n",
        "EPS_DECAY = 200     #무작위성 감소율..\n",
        "\n",
        "GAMMA = 0.8      # 할인계수\n",
        "LR = 0.001       # 학습률\n",
        "BATCH_SIZE = 64  # 배치 크기"
      ],
      "metadata": {
        "id": "yFB4KrVVcjkm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.2 DQN 에이전트\n",
        "\n",
        "- 게임을 하기 위한 신경망\n",
        "- 행동을 결정하기 위한 함수\n",
        "- 각 행동의 가치를 학습하는 함수"
      ],
      "metadata": {
        "id": "dJYadZ-gX9a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력정보 : 카트 위치, 가트 속도, 막대기 각도, 막대기 속도\n",
        "\n",
        "출력정보 : 좌, 우 어디로 갈지.."
      ],
      "metadata": {
        "id": "Cf2bcZ-nj2tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.3 이전 경험 기억하기\n",
        "\n",
        "def memorize(...):\n",
        "\n",
        ": 현재 상태, 행동, 행동에 대한 보상, 행동으로 인해 새로 생성된 상태 를 기억에 추가함.."
      ],
      "metadata": {
        "id": "EPrjnqZtlJ_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.4 행동하기\n",
        "\n",
        "def act(..):\n",
        "\n",
        "- 무작위로 숫자를 골라서 특정 입실론 값보다 크면 학습한 대로 행동하고, 작으면 무작위로 행동한다..\n",
        "- 학습 초기에는 학습이 의미있다고 생각하기 어렵다.. -> 입실론 값을 크게한다. (다양한 경험을 해보도록 한다..)\n",
        "- 학습이 진행될수록 입실론 값을 줄여서 학습한 대로 행동할 수 있도록 한다.."
      ],
      "metadata": {
        "id": "ZdgAY6vPlMmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EPS_THRESHOLD**\n",
        "\n",
        "- $\n",
        "0.05 + 0.85\\exp^{(-(\\text{학습횟수})/200)}\n",
        "$\n",
        "- 0.9에서 0.05로 감소하는 함수 "
      ],
      "metadata": {
        "id": "ofTXSUnJmlid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.5 이전 경험으로부터 배우기\n",
        "\n",
        "- DQN 에이전트가 기억하고 다시 상기할 수 있도록 학습시켜야 한다\n",
        "- **경험 리플레이**"
      ],
      "metadata": {
        "id": "1poR9sEylOZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        #4가지 정보를 입력받아 좌, 우 어디로 갈지 결정..\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "        #최적화 객체,,\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
        "        self.steps_done = 0     #학습할 때마다 증가..\n",
        "        self.memory = deque(maxlen=10000)       #이전 경험의 기억을 담음.. -> 오래된 기억은 자동으로 잊음\n",
        "\n",
        "    #기억을 추가하는 함수..\n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append((state,  #현재 상태\n",
        "                            action, #현재 상태에서의 행동\n",
        "                            torch.FloatTensor([reward]),    #행동에 대한 보상\n",
        "                            torch.FloatTensor([next_state])))   #행동으로 인해 새로 생성된 상태\n",
        "    \n",
        "    #행동하는 함수\n",
        "    def act(self, state):\n",
        "        #임계값 : 0.9 -> 0.05로 감소함..\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "\n",
        "        #임계값보다 크면 학습한대로..\n",
        "        if random.random() > eps_threshold:\n",
        "            #max(1)[1]은 최댓값의 인덱스를 텐서로 반환하고, view를 조지면 [1,1]크기의 2차원 텐서가 된다.\n",
        "            return self.model(state).data.max(1)[1].view(1, 1)\n",
        "        #작으면 무작위로 행동\n",
        "        else:\n",
        "            return torch.LongTensor([[random.randrange(2)]])\n",
        "    \n",
        "    #경험으로 부터 배우는 함수\n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)      #무작위로 경험 가져오기\n",
        "        states, actions, rewards, next_states = zip(*batch) #각 경험을 따로 묶기\n",
        "\n",
        "        #각 경험 준비..\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.cat(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "\n",
        "\n",
        "        current_q = self.model(states).gather(1, actions)\n",
        "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
        "        expected_q = rewards + (GAMMA * max_next_q)\n",
        "        \n",
        "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "GJ6rL3a1e0oK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.2.6 학습 시작하기"
      ],
      "metadata": {
        "id": "2qRfMtZTb9cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "agent = DQNAgent()\n",
        "score_history = []"
      ],
      "metadata": {
        "id": "oPQ2ZTJBfGEM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(1, EPISODES+1):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    while True:\n",
        "        env.render()\n",
        "        state = torch.FloatTensor([state])\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "        # 게임이 끝났을 경우 마이너스 보상주기\n",
        "        if done:\n",
        "            reward = -1\n",
        "\n",
        "        agent.memorize(state, action, reward, next_state)\n",
        "        agent.learn()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
        "            score_history.append(steps)\n",
        "            break"
      ],
      "metadata": {
        "id": "PhzU1EEWfHb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "점수 시각화.."
      ],
      "metadata": {
        "id": "Zk7WeRIKftpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(score_history)\n",
        "plt.ylabel('score')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "HRGyTzb8fv8T",
        "outputId": "958f051b-62b9-4f52-807b-605238fdd366"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.3 마치며"
      ],
      "metadata": {
        "id": "gDW4ygLzb8bS"
      }
    }
  ]
}
